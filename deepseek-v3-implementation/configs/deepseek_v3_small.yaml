# DeepSeek-V3 Small Configuration (for testing on 4-8 GPUs)

model:
  num_layers: 12
  vocab_size: 32000

  mla:
    d_model: 1024
    d_latent: 256
    num_heads: 16
    num_kv_heads: 16
    use_fp8_kv: false
    max_context_length: 8192
    use_flash_mla: true
    use_rope: true
    rope_theta: 10000.0

  moe:
    num_experts: 16
    num_experts_per_token: 2
    expert_intermediate_size: 2048
    num_shared_experts: 0
    shared_intermediate_size: 0

    router_aux_loss_weight: 0.001
    router_temperature: 1.0
    router_noise_std: 0.05
    capacity_factor: 1.0

    use_aux_loss_free: false
    use_deep_ep: false  # Not needed for small scale

  norm_type: "rmsnorm"
  norm_eps: 1.0e-6
  tie_word_embeddings: false
  init_method_std: 0.006

parallel:
  tensor_parallel_size: 2
  pipeline_parallel_size: 1
  expert_parallel_size: 1
  data_parallel_size: -1
  zero_stage: 1
  zero_offload: false

training:
  global_batch_size: 128
  micro_batch_size: 2
  seq_length: 1024

  # Chinchilla-optimal scaling (REQ-T2P-1, REQ-T2P-2)
  # Small model: ~250M active params → 5B tokens @20 T/P, 6.5B @26 T/P
  # Tokens/step: 128 × 1024 = 131,072
  # Required steps: 5B / 131K ≈ 38,147 steps
  tokens_per_parameter_ratio: 20.0  # Conservative Chinchilla optimum
  total_training_tokens: 5000000000  # 5B tokens (250M × 20)

  learning_rate: 3.0e-4
  min_learning_rate: 3.0e-5
  lr_warmup_steps: 500
  lr_decay_style: "cosine"
  weight_decay: 0.1
  grad_clip: 1.0

  use_fp16: false
  use_bf16: true
  use_fp8: false

  use_mtp: false

  train_steps: 38147  # Updated for Chinchilla compliance
  eval_interval: 500
  save_interval: 1000
  log_interval: 10

  optimizer: "adamw"
  adam_beta1: 0.9
  adam_beta2: 0.95
  adam_eps: 1.0e-8

data:
  train_data_path: "./data/train"
  val_data_path: "./data/val"
  num_workers: 2

checkpoint:
  save_dir: "./checkpoints_small"
  keep_last_n: 3

logging:
  output_dir: "./outputs_small"
  use_wandb: false
  log_level: "INFO"
