# DeepSeek-V3 Base Configuration (671B parameters, 37B active)
# Optimized for 32+ H100 GPUs

model:
  # Architecture
  num_layers: 61
  vocab_size: 128000

  # MLA (Multi-head Latent Attention)
  mla:
    d_model: 7168
    d_latent: 1536  # ~1/5 of d_model for KV compression
    num_heads: 128
    num_kv_heads: 128
    use_fp8_kv: true
    max_context_length: 128000
    use_flash_mla: true
    flash_mla_backend: "auto"
    use_rope: true
    rope_theta: 10000.0

  # MoE (Mixture of Experts)
  moe:
    num_experts: 256
    num_experts_per_token: 8  # Top-8 routing
    expert_intermediate_size: 18432
    num_shared_experts: 2  # Always-active shared experts
    shared_intermediate_size: 18432

    # Routing
    router_aux_loss_weight: 0.0  # Using aux-loss-free
    router_temperature: 1.0
    router_noise_std: 0.1  # Anneal to 0
    capacity_factor: 1.0

    # Load balancing
    use_aux_loss_free: true
    balance_loss_type: "entropy"

    # DeepEP
    use_deep_ep: true
    deep_ep_fp8: true
    deep_ep_async: true

  # Normalization
  norm_type: "rmsnorm"
  norm_eps: 1.0e-6

  # Embeddings
  tie_word_embeddings: false

  # Initialization
  init_method_std: 0.006

# Parallelism
parallel:
  tensor_parallel_size: 4
  pipeline_parallel_size: 4
  expert_parallel_size: 2
  data_parallel_size: -1  # Auto-computed

  # ZeRO optimization
  zero_stage: 1
  zero_offload: false

  # Communication
  overlap_grad_reduce: true
  overlap_param_gather: true

# Training
training:
  # Batch settings
  global_batch_size: 4096
  micro_batch_size: 1
  seq_length: 4096

  # Chinchilla-optimal scaling (REQ-T2P-1, REQ-T2P-2)
  # 671B model: ~150.46B active params → 3009.3B tokens @20 T/P, 3912.0B @26 T/P
  # Tokens/step: 4096 × 4096 = 16,777,216
  # Required steps: 3009.3B / 16.78M ≈ 179,391 steps
  # Note: DeepSeek-V3 paper uses ~14.8T tokens for even better quality
  tokens_per_parameter_ratio: 20.0  # Conservative Chinchilla optimum
  total_training_tokens: 3009300000000  # 3009.3B tokens (150.46B × 20)

  # Optimization
  learning_rate: 1.8e-4
  min_learning_rate: 1.8e-5
  lr_warmup_steps: 2000
  lr_decay_style: "cosine"
  weight_decay: 0.1
  grad_clip: 1.0

  # Mixed precision
  use_fp16: false
  use_bf16: true
  use_fp8: true

  # Multi-token prediction
  use_mtp: true
  num_predict_tokens: 2

  # Schedule
  train_steps: 179391  # Updated for Chinchilla compliance
  eval_interval: 1000
  save_interval: 5000
  log_interval: 10

  # Optimizer
  optimizer: "adamw"
  adam_beta1: 0.9
  adam_beta2: 0.95
  adam_eps: 1.0e-8

# Data
data:
  train_data_path: "/path/to/train_data"
  val_data_path: "/path/to/val_data"
  num_workers: 4
  prefetch_factor: 2

# Checkpointing
checkpoint:
  save_dir: "./checkpoints"
  keep_last_n: 5
  save_optimizer_states: true

# Logging
logging:
  output_dir: "./outputs"
  use_wandb: true
  wandb_project: "deepseek-v3"
  wandb_entity: null
  log_level: "INFO"
